{
  "name": "recommended",
  "description": "Balanced defaults for most LoRA fine-tuning tasks",
  "rank": 64,
  "alpha": 128,
  "dropout": 0.1,
  "target_modules_str": "q_proj k_proj v_proj o_proj",
  "attention_type": "both",
  "bias": "none",
  "learning_rate": 1e-4,
  "batch_size": 1,
  "gradient_accumulation": 4,
  "epochs": 100,
  "warmup_steps": 100,
  "weight_decay": 0.01,
  "max_grad_norm": 1.0,
  "seed": 42,
  "shift": 3.0,
  "num_inference_steps": 8,
  "optimizer_type": "adamw",
  "scheduler_type": "cosine",
  "cfg_ratio": 0.15,
  "save_every": 10,
  "log_every": 10,
  "log_heavy_every": 50,
  "gradient_checkpointing": true,
  "offload_encoder": false,
  "sample_every_n_epochs": 0
}
